import os
import glob
import asyncio
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn import metrics
import joblib
import numpy as np

from src.core.patterns import get_compiled_patterns
from src.core.link_hunter import LinkHunter
from src.core.whitelist import Whitelist
from src.core.ner import EntityExtractor
from src.core.ner import EntityExtractor
from src.core.features import FeatureExtractor
from src.core.sentiment import SentimentAnalyzer
from src.utils.text_processing import clean_text, normalize_homoglyphs, generate_homoglyphs
from src import config
import logging

try:
    import torch
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("Transformers –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF fallback.")

try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    logging.warning("ONNX Runtime –Ω–µ –Ω–∞–π–¥–µ–Ω.")

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, roc_curve
from src.core.explainer import ExplainabilityEngine

class GuardianClassifier:
    def __init__(self, model_path=None):
        self.regex_patterns = get_compiled_patterns()
        self.link_hunter = LinkHunter()
        self.whitelist = Whitelist()
        self.ner = EntityExtractor()
        self.feature_extractor = FeatureExtractor()
        self.sentiment = SentimentAnalyzer()
        self.model_path = model_path if model_path else config.MODEL_PATH
        self.use_bert = TRANSFORMERS_AVAILABLE
        self.use_onnx = ONNX_AVAILABLE and os.path.exists(config.ONNX_MODEL_PATH)
        self.threshold = config.SKLEARN_THRESHOLD_DEFAULT
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
        self.explainer = ExplainabilityEngine(self)
        
        if self.use_bert:
            # –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å: BERT + –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self.clf = RandomForestClassifier(n_estimators=100, random_state=42)
            self.tokenizer = None
            self.bert_model = None
            self.ort_session = None
        else:
            self.ml_pipeline = Pipeline([
                ('tfidf', TfidfVectorizer(ngram_range=(1, 3), analyzer='char_wb', max_features=5000)),
                ('clf', SGDClassifier(loss='log_loss', random_state=42)),
            ])
        
        if os.path.exists(self.model_path):
            self.load_model()
        else:
            self.is_trained = False

    def _init_bert(self):
        if self.tokenizer is None:
            logging.info("–ó–∞–≥—Ä—É–∑–∫–∞ RuBERT...")
            model_name = "cointegrated/rubert-tiny2"
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            if self.use_onnx:
                logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ ONNX –º–æ–¥–µ–ª–∏ ({config.ONNX_MODEL_PATH})...")
                try:
                    self.ort_session = ort.InferenceSession(config.ONNX_MODEL_PATH)
                    logging.info("ONNX Runtime –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ! üöÄ")
                except Exception as e:
                    logging.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ONNX: {e}. –û—Ç–∫–∞—Ç –∫ PyTorch.")
                    self.use_onnx = False
            
            if not self.use_onnx:
                self.bert_model = AutoModel.from_pretrained(model_name)
                # –ó–∞–º–æ—Ä–æ–∑–∫–∞ BERT –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞–∫ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                for param in self.bert_model.parameters():
                    param.requires_grad = False
                logging.info("RuBERT (PyTorch) –∑–∞–≥—Ä—É–∂–µ–Ω.")

    def _get_bert_embeddings(self, texts):
        if not self.use_bert:
            return None
        self._init_bert()
        
        embeddings = []
        # –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±—ã–ª–∞ –±—ã –ª—É—á—à–µ, –Ω–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ–¥–æ–π–¥–µ—Ç –∏ —Ü–∏–∫–ª
        for text in texts:
            t = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=512)
            
            if self.use_onnx:
                # ONNX Inference (–õ–æ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ —á–µ—Ä–µ–∑ ONNX)
                ort_inputs = {
                    'input_ids': t['input_ids'].numpy(),
                    'attention_mask': t['attention_mask'].numpy()
                }
                ort_outs = self.ort_session.run(None, ort_inputs)
                # –í—ã—Ö–æ–¥ 0 - —ç—Ç–æ last_hidden_state
                emb = ort_outs[0][:, 0, :] # –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç —Ç–æ–∫–µ–Ω–∞ CLS
                embeddings.append(emb[0])
            else:
                # PyTorch Inference (–õ–æ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ —á–µ—Ä–µ–∑ PyTorch)
                with torch.no_grad():
                    model_output = self.bert_model(**t)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–æ–∫–µ–Ω–∞ CLS (–∏–Ω–¥–µ–∫—Å 0)
                emb = model_output.last_hidden_state[:, 0, :]
                embeddings.append(emb[0].numpy())
                
        return embeddings

    def save_model(self):
        if self.use_bert:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏ –ø–æ—Ä–æ–≥–∞
            # Joblib –¥–∞–º–ø–∏—Ç –æ–±—ä–µ–∫—Ç, –ø–æ—ç—Ç–æ–º—É self.threshold (—á–ª–µ–Ω –∫–ª–∞—Å—Å–∞) —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è, –µ—Å–ª–∏ –º—ã –¥–∞–º–ø–∏–º self.clf? 
            # –ù–µ—Ç, self.clf - —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –æ–±—ä–µ–∫—Ç sklearn. 
            # –ù–∞–º –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞.
            # –ò–¥–µ–∞–ª—å–Ω–æ –±—ã–ª–æ –±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–µ—Å—å —ç–∫–∑–µ–º–ø–ª—è—Ä GuardianClassifier, –ù–û –æ–Ω –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–µ–ø–∏–∫–ª–∏—Ä—É–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã.
            # Re.Pattern –ø–∏–∫–ª–∏—Ä—É–µ—Ç—Å—è –≤ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö python. 
            # –ù–æ –¥–∞–≤–∞–π—Ç–µ –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ sklearn, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞.
            # –ú—ã –º–æ–∂–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä–æ–≥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ñ–∞–π–ª–µ –∏–ª–∏ "–≤–∑–ª–æ–º–∞—Ç—å" –æ–±—ä–µ–∫—Ç, –µ—Å–ª–∏ –±—ã –º—ã –¥–∞–º–ø–∏–ª–∏ self.
            # –ü–æ–∫–∞ —á—Ç–æ –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å, —á—Ç–æ –ø–æ–ª–∞–≥–∞–µ–º—Å—è –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏–ª–∏ –∂–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç, –ò–õ–ò —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –º–æ–¥–µ–ª—å—é –∏ –ø–æ—Ä–æ–≥–æ–º.
            state = {
                'model': self.clf,
                'threshold': self.threshold
            }
            joblib.dump(state, self.model_path)
            # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –≠—Ç–æ –º–µ–Ω—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞! load_model –¥–æ–ª–∂–Ω–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è.
        else:
            joblib.dump(self.ml_pipeline, self.model_path)
        logging.info(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.model_path}")
        
    def load_model(self):
        try:
            loaded_obj = joblib.load(self.model_path)
            
            if self.use_bert:
                if isinstance(loaded_obj, dict) and 'model' in loaded_obj:
                    self.clf = loaded_obj['model']
                    self.threshold = loaded_obj.get('threshold', 0.6)
                else:
                    # –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ (—Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å)
                    self.clf = loaded_obj
                    self.threshold = 0.6
                
                # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ BERT —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
                self._init_bert()
            else:
                self.ml_pipeline = loaded_obj
                
            self.is_trained = True
            logging.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {self.model_path} (Threshold: {self.threshold:.4f})")
        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
            self.is_trained = False
            self.threshold = config.SKLEARN_THRESHOLD_DEFAULT # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (Fallback)

    def check_keywords(self, text: str) -> list[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ä–∞–±–æ—Ç–∞–≤—à–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        triggers = []
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        text_norm = normalize_homoglyphs(text)
        
        for pattern in self.regex_patterns:
            if pattern.search(text_norm): # –ò—â–µ–º –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
                triggers.append(pattern.pattern)
        return triggers

    def check_safe_patterns(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–µ—Ä–≤–∏—Å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–∫–æ–¥—ã, –ø–∞—Ä–æ–ª–∏), –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –Ω–∞–¥–æ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å"""
        safe_triggers = [
            r"–∫–æ–¥\s*[:\-]?\s*\d+",
            r"code\s*[:\-]?\s*\d+",
            r"–ø–∞—Ä–æ–ª—å\s*[:\-]?\s*\d+",
            r"password\s*[:\-]?\s*\d+",
            r"–Ω–∏–∫–æ–º—É –Ω–µ —Å–æ–æ–±—â–∞–π—Ç–µ",
            r"don'?t share",
            r"–≤–∞—à –∫–æ–¥",
            r"your code"
        ]
        import re
        for pattern in safe_triggers:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        return False

    def load_dataset(self, dataset_path: str):
        scam_files = glob.glob(os.path.join(dataset_path, "scam_*.txt"))
        safe_files = glob.glob(os.path.join(dataset_path, "safe_*.txt"))
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã
        scam_path = os.path.join(dataset_path, "scam_samples.txt")
        safe_path = os.path.join(dataset_path, "safe_samples.txt")
        
        # –°–º–µ—à–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º "phishing_dataset_*.txt")
        mixed_files = glob.glob(os.path.join(dataset_path, "phishing_dataset_*.txt"))
        
        data = []
        labels = [] # 1 - scam (–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ), 0 - safe (–±–µ–∑–æ–ø–∞—Å–Ω–æ)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (scam_*.txt)
        for f_path in scam_files:
            base_name = os.path.basename(f_path)
            logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ Scam –¥–∞—Ç–∞—Å–µ—Ç–∞: {base_name}")
            
            # OVERSAMPLING (–ü–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è) –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ / —É—Å–∏–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
            multiplier = 1
            if "enrichment" in base_name or "samples" in base_name:
                multiplier = 50 
                logging.info(f"  -> –ü—Ä–∏–º–µ–Ω–µ–Ω –±—É—Å—Ç x{multiplier}")

            try:
                with open(f_path, 'r', encoding='utf-8') as f:
                    file_lines = f.readlines()
                    for _ in range(multiplier):
                        for line in file_lines:
                            if line.strip():
                                data.append(line.strip())
                                labels.append(1)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {f_path}: {e}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (safe_*.txt)
        for f_path in safe_files:
            base_name = os.path.basename(f_path)
            logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ Safe –¥–∞—Ç–∞—Å–µ—Ç–∞: {base_name}")
            
            # OVERSAMPLING (–ü–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è) –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–æ–∂–µ
            multiplier = 1
            if "enrichment" in base_name or "samples" in base_name:
                multiplier = 50
                logging.info(f"  -> –ü—Ä–∏–º–µ–Ω–µ–Ω –±—É—Å—Ç x{multiplier}")

            try:
                with open(f_path, 'r', encoding='utf-8') as f:
                    file_lines = f.readlines()
                    for _ in range(multiplier):
                        for line in file_lines:
                            if line.strip():
                                data.append(line.strip())
                                labels.append(0)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {f_path}: {e}")
                        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        for m_file in mixed_files:
            logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {os.path.basename(m_file)}")
            try:
                with open(m_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if not line: continue
                        
                        # –§–æ—Ä–º–∞—Ç: 'Scam: "Text"' –∏–ª–∏ 'Safe: "Text"'
                        if line.startswith("Scam:"):
                            content = line[5:].strip().strip('"')
                            data.append(content)
                            labels.append(1)
                        elif line.startswith("Safe:"):
                            content = line[5:].strip().strip('"')
                            data.append(content)
                            labels.append(0)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {m_file}: {e}")
                        
        return data, labels

    def train(self, dataset_path: str):
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        X, y = self.load_dataset(dataset_path)
        if not X:
            print("–î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç!")
            return
            
        print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(X)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (Homoglyphs)
        print("–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (Homoglyphs)...")
        aug_X = []
        aug_y = []
        for txt, lbl in zip(X_train, y_train):
            if lbl == 1: # –¢–æ–ª—å–∫–æ –¥–ª—è SCAM
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 50%
                import random
                if random.random() < 0.5:
                    spoofed = generate_homoglyphs(txt)
                    if spoofed != txt:
                        aug_X.append(spoofed)
                        aug_y.append(1)
        
        if aug_X:
            print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(aug_X)} –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –≥–æ–º–æ–≥–ª–∏—Ñ–∞–º–∏.")
            X_train.extend(aug_X)
            y_train.extend(aug_y)
        
        print(f"–û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–∏ (BERT={self.use_bert})...")
        
        if self.use_bert:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
            X_train_emb = self._get_bert_embeddings(X_train)
            X_test_emb = self._get_bert_embeddings(X_test)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            X_train_stats = np.array([self.feature_extractor.extract(t) for t in X_train])
            X_test_stats = np.array([self.feature_extractor.extract(t) for t in X_test])
            
            # –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π
            print("–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
            def get_sent(texts):
                res = []
                for i, t in enumerate(texts):
                    if i % 1000 == 0 and i > 0: print(f"  Processed {i}/{len(texts)}")
                    s = self.sentiment.analyze(t)
                    res.append([s['negative'], s['neutral'], s['positive']])
                return np.array(res)

            X_train_sent = get_sent(X_train)
            X_test_sent = get_sent(X_test)

            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            print("–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            X_train_combined = np.hstack((X_train_emb, X_train_stats, X_train_sent))
            X_test_combined = np.hstack((X_test_emb, X_test_stats, X_test_sent))
            
            self.clf.fit(X_train_combined, y_train)
            
            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
            y_pred_proba = self.clf.predict_proba(X_test_combined)[:, 1]
            
            # –ê–≤—Ç–æ-–∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø–æ—Ä–æ–≥–∞
            fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
            if len(thresholds) > 0:
                J = tpr - fpr
                ix = np.argmax(J)
                best_thresh = thresholds[ix]
                print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ (Best Threshold): {best_thresh:.4f}")
                self.threshold = float(best_thresh) # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–ª–∞—Å—Å
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞
            y_pred = (y_pred_proba >= self.threshold).astype(int)
            
            print("\n--- –û–¢–ß–ï–¢ –û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò ---")
            print(classification_report(y_test, y_pred, target_names=['Safe', 'Scam']))
            print(f"Precision: {precision_score(y_test, y_pred):.4f}")
            print(f"Recall:    {recall_score(y_test, y_pred):.4f} (–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ!)")
            print(f"F1-Score:  {f1_score(y_test, y_pred):.4f}")
            print("-------------------------------\n")
            
        else:
            # TF-IDF Pipeline
            X_train_clean = [clean_text(t) for t in X_train]
            X_test_clean = [clean_text(t) for t in X_test]
            
            self.ml_pipeline.fit(X_train_clean, y_train)
            predicted = self.ml_pipeline.predict(X_test_clean)
            print(f"–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (TF-IDF): {metrics.accuracy_score(y_test, predicted):.4f}")

        self.is_trained = True
        self.save_model()

    def predict(self, text: str, strict_mode: bool = False, context: list[str] = None) -> dict:
        # 0. Whitelist check
        if self.whitelist.is_trusted(text):
             return {
                "text": text,
                "is_scam": False,
                "triggers": [],
                "ml_score": 0.0,
                "ml_verdict": "Safe",
                "reason": "–í –±–µ–ª–æ–º —Å–ø–∏—Å–∫–µ (–î–æ–≤–µ—Ä–µ–Ω–Ω—ã–π)",
                "link_analysis": {"score": 0.0, "has_links": False}
            }

        # Normalize homoglyphs
        text_norm = normalize_homoglyphs(text)
        text_clean = clean_text(text_norm)
        
        # 1. Regex check (Scam Triggers) - ONLY on current text
        triggers = self.check_keywords(text) 
        
        # 1.1 Safe Pattern check -- ONLY IF NOT STRICT
        is_safe_pattern = False
        if not strict_mode:
            is_safe_pattern = self.check_safe_patterns(text)

        # 2. ML check - Uses Context!
        ml_score = 0.0
        ml_verdict = "Unknown"
        
        # Prepare text for ML: Context + Current
        ml_input_text = text_clean
        if context:
            # Join with [SEP] token
            clean_context = [clean_text(normalize_homoglyphs(c)) for c in context]
            ml_input_text = " [SEP] ".join(clean_context + [text_clean])
        
        if self.is_trained:
            try:
                if self.use_bert:
                     self._init_bert()
                     # 1. Embeddings on Context + Text
                     text_emb = self._get_bert_embeddings([ml_input_text]) 
                     
                     # 2. Statistics (Manual features) - Only on CURRENT text
                     text_stats = self.feature_extractor.extract(text)
                     text_stats = text_stats.reshape(1, -1)
                     
                     # 3. Sentiment
                     sent = self.sentiment.analyze(text)
                     text_sent = np.array([[sent['negative'], sent['neutral'], sent['positive']]])

                     # 4. Combine
                     combined_features = np.hstack((text_emb, text_stats, text_sent))
                     
                     probs = self.clf.predict_proba(combined_features)[0]
                else:
                    # TF-IDF
                    probs = self.ml_pipeline.predict_proba([ml_input_text])[0]
                
                ml_score = probs[1]
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ ML: {e}")
                ml_score = 0.0
                
            # Dynamic threshold usage
            ml_verdict = "Scam" if ml_score > self.threshold else "Safe"
            
        # 3. Link Hunter check
        link_analysis = self.link_hunter.analyze(text)
        link_score = link_analysis['score']
        
        # Final decision logic
        final_score = max(ml_score, link_score)
        
        # Default logic uses calibrated ML threshold
        is_scam = bool(bool(triggers) or (ml_score > self.threshold) or (link_score > 0.7))
        reason = "–ë–µ–∑–æ–ø–∞—Å–Ω–æ"
        
        # NER Check
        entities = self.ner.extract(text_norm)
        
        # --- NER Heuristics ---
        # 1. Fake Authority / Impersonation (MVD, FSB + Urgency/Money)
        risky_orgs = ['–ú–í–î', '–§–°–ë', '–ü–æ–ª–∏—Ü–∏—è', '–¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫', '–ì–æ—Å—É—Å–ª—É–≥–∏', '–°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–º–∏—Ç–µ—Ç']
        found_risky_org = [org for org in entities.get('ORG', []) if any(r.lower() in org.lower() for r in risky_orgs)]
        
        if found_risky_org:
            # If Authority is mentioned, we need Urgency or Sensitive info to call it scam
            if entities.get('URGENCY') or entities.get('SENSITIVE') or "–ø–µ—Ä–µ–≤–æ–¥" in text_norm.lower():
                is_scam = True
                reason = f"–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ—Ç –∏–º–µ–Ω–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {found_risky_org[0]}"
                # Force high score for UI
                final_score = max(final_score, 0.95)

        if is_safe_pattern:
            if link_score < 0.8: 
                is_scam = False
                reason = "–ë–µ–∑–æ–ø–∞—Å–Ω–æ (–°–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ)"
        
        if is_scam:
            if found_risky_org and "–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å" in reason:
                 pass # Reason already set
            elif triggers:
                reason = "–°—Ä–∞–±–æ—Ç–∞–ª —Ç—Ä–∏–≥–≥–µ—Ä (–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)"
            elif link_score > 0.7:
                 reason = f"–§–∏—à–∏–Ω–≥–æ–≤–∞—è —Å—Å—ã–ª–∫–∞: {', '.join([r['reasons'][0] for r in link_analysis['suspicious_links']])}"
            elif ml_score > self.threshold:
                reason = "–í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ML"

        # Explainability (Only if not strict to avoid infinite loops)
        explanation = []
        if not strict_mode: 
            explanation = self.explainer.explain(text, final_score, triggers, entities)

        result = {
            "text": text,
            "is_scam": is_scam,
            "triggers": triggers,
            "ml_score": float(round(final_score, 4)),
            "ml_verdict": ml_verdict,
            "reason": reason,
            "link_analysis": link_analysis,
            "entities": entities,
            "explanation": explanation
        }
        return result

    async def predict_async(self, text: str, strict_mode: bool = False, context: list[str] = None) -> dict:
        """Asynchronous wrapper for predict"""
        return await asyncio.to_thread(self.predict, text, strict_mode, context)
